# -*- coding: utf-8 -*-
"""545project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LOwkd9SO8zh7dKim7uEx3O0IHZRi0Gnw
"""

import math
import torch.nn as nn
import torch.nn.functional as F
import torch
from torch.optim import SGD
from torch.utils.data import Dataset, DataLoader
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from PIL import Image
import os, random, pickle
from os.path import join, isfile
from tqdm import tqdm
import numpy as np

class CNN(nn.Module):
  def __init__(self, num_class):
    super().__init__()
    self.block1 = self.conv_block(3, out_dim = 128)
    self.block2 = self.conv_block(128, out_dim = 128)
    self.block3 = self.conv_block(128, 128)


    self.block4 = self.conv_block(128, 256)
    self.block5 = self.conv_block(256, 256)
    self.block6 = self.conv_block(256, 128)

    self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout2d()])
    self.fc = nn.Sequential(nn.Linear(128, num_class))
    self.average_pool = nn.AdaptiveAvgPool2d((1, 1))

  def conv_block(self, input_dim, out_dim, kernel_size=3, stride=2, padding=2):  # 原来的是0.01 stride = 1, padding = 1, 用的是leakyrelu
    return nn.Sequential(
        nn.Conv2d(input_dim, out_dim, kernel_size, stride, padding, bias=False),
        nn.BatchNorm2d(out_dim),  # 防止 saturation
        nn.ReLU(inplace=True))
    
  def forward(self, x):
    z = self.block1(x)
    z = self.block2(z)
    z = self.block3(z)
    z = self.maxpool(z)
    z = self.block4(z)
    z = self.block5(z)
    z = self.block6(z)
    z = self.average_pool(z)
    temp = z.shape[0]
    z = z.view(temp, -1)
    z = self.fc(z)
    return z

def get_class_balanced_labels(targets, labels_per_class, save_path=None):
  num_classes = max(targets) + 1

  indices = list(range(len(targets)))
  random.shuffle(indices)

  label_count = {i: 0 for i in range(num_classes)}
  label_indices, unlabel_indices = [], []
  for idx in indices:
    if label_count[targets[idx].item()] < labels_per_class:
        label_indices.append(idx)
        label_count[targets[idx].item()] += 1
    else:
        unlabel_indices.append(idx)

  if save_path is not None:
    with open(join(save_path, 'label_indices.txt'), 'w') as f:
        for idx in label_indices:
            f.write(str(idx) + '\n')

  return label_indices, unlabel_indices


def get_repeated_indices(indices, num_iters, batch_size):
    length = num_iters * batch_size
    num_epochs = length // len(indices) + 1
    repeated_indices = []

    for epoch in tqdm(range(num_epochs), desc='Pre-allocating indices'):
        random.shuffle(indices)
        repeated_indices += indices

    return repeated_indices[:length]




class MNIST(dsets.MNIST):   # need to tanspose or not
  num_classes = 10
  def __init__(self, num_labels, num_iters, batch_size, return_unlabel=True, save_path=None, **kwargs):
    super(MNIST, self).__init__(**kwargs)
    labels_per_class = num_labels // self.num_classes
    self.return_unlabel = return_unlabel
    self.label_indices, self.unlabel_indices = get_class_balanced_labels(self.targets, labels_per_class, save_path)

    self.repeated_label_indices = get_repeated_indices(self.label_indices, num_iters, batch_size)   # ????? 不理解，后续查看能不能修改

    self.repeated_unlabel_indices = get_repeated_indices(self.unlabel_indices, num_iters, batch_size)

  def __len__(self):
      return len(self.repeated_label_indices)

  def __getitem__(self, idx):
    label_idx = self.repeated_label_indices[idx]
    label_img = self.data[label_idx]
    label_target = self.targets[label_idx]
    label_img = np.asarray(label_img)
    label_img = Image.fromarray(label_img)
    label_img = self.transform(label_img)
    print(type(label_target))
    label_target = self.target_transform(label_target)

    unlabel_idx = self.repeated_unlabel_indices[idx]
    unlabel_img = self.data[unlabel_idx]
    unlabel_target = self.targets[unlabel_idx]
    unlabel_img = np.asarray(unlabel_img)
    unlabel_img = Image.fromarray(unlabel_img)
    unlabel_img = self.transform(unlabel_img)
    unlabel_target = self.target_transform(unlabel_target)

    return label_img, label_target, unlabel_img, unlabel_target

train_transform = {'mnist':
                transforms.Compose([
                transforms.RandomCrop(32, padding=4),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor()
                #transforms.Normalize((0.1307,), (0.3081,))
                ])
                }
MNIST_keywords = {'train': True, 'download': True}
test_keywords = {'train': False, 'download': True}
def dataloader(path, bs, num_workers, num_labels, num_iters, return_unlabel=True, save_path=None):

  train_dataset = MNIST(
          root = path,
          num_labels = num_labels,
          num_iters = num_iters,
          batch_size = bs,
          return_unlabel = return_unlabel,
          transform = train_transform['mnist'],
          save_path = save_path,
          **MNIST_keywords
  )
  train_loader = DataLoader(train_dataset, batch_size=bs, num_workers=num_workers, shuffle=False)

  test_transform = transforms.Compose([
          transforms.ToTensor()
          #transforms.Normalize((0.1307,), (0.3081,))
  ])
  test_dataset = dsets.MNIST(root='E:\\EECS545_Project\\', transform=test_transform, **test_keywords)
  test_loader = DataLoader(test_dataset, batch_size=100, num_workers=num_workers, shuffle=False)

  return iter(train_loader), test_loader




if __name__ == '__main__':
    train_loader, test_loader = dataloader(
        path='E://EECS545_Project//',
        bs=1024,
        num_workers=8,
        num_labels=2000,
        num_iters=40000,
        return_unlabel=True,
        save_path='E://EECS545_Project//')
    a, b, c, d = next(train_loader)
    print(a)
# model = CNN(num_class=10)
# model.cuda()
# lr = 0.1
# momentum = 0.8
# weight_decay = 1e-4
# optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay = weight_decay)   # 原来momentum = 0.9
#
# def compute_lr(step):
#   if step <= 4000:
#     lr = lr * step / 4000
#   else:
#     lr = lr
#     for milestone in [300000, 350000]:
#       lr = lr * 0.1
#   return lr
#
# for iter in range(400000):
#   label_img, label_target, unlabel_img, unlabel_target = next(train_loader)
#
#   if torch.cuda.is_available():
#     label_img.cuda()
#     label_target.cuda()
#     unlabel_img.cuda()
#     unlabel_target.cuda()
#   # label_gt_new = F.one_hot(label_target, num_classes=args.num_classes).float()
#
#   lr = compute_lr(iter)
#
#   label_pred = model(label_img)
#   label_loss = F.cross_entropy(label_pred, label_gt)
#
#   theta_gradient = torch.autograd.grad(label_loss, model.parameters(), only_inputs=True)
#   print(theta_gradient)